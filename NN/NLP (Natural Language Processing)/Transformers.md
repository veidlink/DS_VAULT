## üìö References 
- Tags :  [[Encoder & Decoder]] [[NLP (Natural language processing)]] [[Seq2Seq]]
- Links: [–í–∏–¥–µ–æ —Å –ò–ê–î](https://www.youtube.com/watch?v=P5wNb9Mt9RE)

## ‚ùì Questions
- 

## üîó Related material

# Transformers - Seq2Seq  –±–µ–∑ RNN.

 –í –æ–±–æ–±—â–µ–Ω–Ω–æ–º –≤–∏–¥–µ:
 ![a](imgs/31.png)
 –ü–æ—ç—Ç–∞–ø–Ω–æ —Ä–∞–∑–±–µ—Ä–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É.

## –ó–∞–≥–ª—è–Ω–µ–º –≤–Ω—É—Ç—Ä—å —ç–Ω–∫–æ–¥–µ—Ä–∞

 ![b](imgs/32.png)
–ù–∞ –≤—Ö–æ–¥ –ø—Ä–∏—Ö–æ–¥–∏—Ç –¥–≤–∞ hidden state, —Å—á–∏—Ç–∞–µ—Ç—Å—è self-attention –∫–∞–∂–¥–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞, –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤–µ–∫—Ç–æ—Ä–∞, –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–µ–º—É —ç–Ω–∫–æ–¥–µ—Ä—É.

–ë–æ–ª—å—à–µ —ç–Ω–∫–æ–¥–µ—Ä–æ–≤ - –±–æ–ª—å—à–µ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏, –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—ã—è–≤–ª—è–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞.

### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è self-attention –≤ transformers ![c](imgs/33.png)
–í self-attention –º—ã —Å—á–∏—Ç–∞–µ–º —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ (–∏–ª–∏ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π attention) –º–µ–∂–¥—É —Å–∞–º–∏–º–∏ hidden states. –ü–æ—Ç–æ–º –≤ —Å–æ—Ñ—Ç–º–∞–∫—Å, —Å—á–∏—Ç–∞–µ–º –≤–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –æ–±–Ω–æ–≤–ª—è–µ–º hidden states. **–ó–¥–µ—Å—å –Ω–∏—á–µ–≥–æ –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è.** –ü—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–∏–ª–∏—Å—å –ª–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ —Å –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –≤–µ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–∞–≤—è—Ç—Å—è –ø–µ—Ä–µ–¥ hidden states –∏ —è–≤–ª—è—é—Ç—Å—è –æ–±—É—á–∞–µ–º—ã–º–∏. –≠—Ç–æ –ª–∏—à—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ, –≥–ª–æ–±–∞–ª—å–Ω–æ –Ω–µ –º–µ–Ω—è—é—â–µ–µ –Ω–∏—á–µ–≥–æ.

#### –ö–∞–∫ —Å—á–∏—Ç–∞–µ—Ç—Å—è context vector –≤ transformers
 ![d](imgs/34.png)
$Z_{1}$ —Ç—É—Ç –≤–µ–∫—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å–∞, –æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–π –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π —Å—É–º–º–æ–π $v_{1} + v_{2}$, –≥–¥–µ –≤–µ—Å–∞ —ç—Ç–æ - –≤–∞–∂–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞ –ø–æ–ª—É—á–µ–Ω–Ω–∞—è —Å–æ—Ñ—Ç–º–∞–∫—Å–æ–º.

#### –†–µ–∞–ª–∏–∑—É–µ–º Multi-headed SA
–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—ã–≥–æ–¥–æ–π MDSA: –∫–∞–∂–¥—ã–π self-attention —É–ª–∞–≤–∏–≤–∞–µ—Ç –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω–æ–≥–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞ –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏.
 ![e](imgs/35.png)
 –í—Å–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ attentions –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É—é—Ç—Å—è (–∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ –º–∞—Ç—Ä–∏—Ü–µ - –æ–¥–∏–Ω hidden state), –ø—Ä–æ—Ö–æ–¥—è—Ç –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π —Å —Ç—Ä–µ–Ω–∏—Ä—É–µ–º—ã–º–∏ –≤–µ—Å–∞–º–∏ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ $Loss$. –ù–∞ –≤—ã—Ö–æ–¥–µ –∏–º–µ–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –Ω–æ–≤—ã—Ö —Å–≤–µ—Ä—Ö hidden states, —É–ª–∞–≤–ª–∏–≤–∞—é—â–∏—Ö —Ü–µ–ª—ã—Ö 8 –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π.

### –î–µ–ª–∞–µ–º Positional Encoding
–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç $RNN$, –∑–¥–µ—Å—å –Ω–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–≤ –∏–ª–∏ –∫–∞–∫–æ–π-—Ç–æ –∏—Å—Ç–æ—Ä–∏–∏, –æ–Ω–∏ –∑–∞–∫–∏–¥—ã–≤–∞—é—Ç—Å—è –≤ —Å–ª–æ–∏ –∏ –∏–¥—É—Ç –ø–æ –Ω–∏–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ -> –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ—Ä—è–¥–æ–∫.

–≠—Ç–æ —Ñ–∏–∫—Å–∏—Ç [[Positional Encoding]].

–°—É—Ç—å –≤ —Ç–æ–º, —á—Ç–æ –º—ã –ø–µ—Ä–µ–¥ –∑–∞–±—Ä–∞—Å—ã–≤–∞–Ω–∏–µ–º hidden states –≤ encoding layers –ø—Ä–∏–±–∞–≤–ª—è–µ–º –∫ hiddem states –≤–µ–∫—Ç–æ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–µ —á—Ç–æ-—Ç–æ –≥–æ–≤–æ—Ä—è—Ç –Ω–∞–º –æ –ø–æ–∑–∏—Ü–∏–∏ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ.
 ![f](imgs/36.png)
### –û–±—â–∞—è —Å—Ö–µ–º–∞ —ç–Ω–∫–æ–¥–µ—Ä–∞
![g](imgs/37.png)
1. –ü—Ä–∏–Ω–∏–º–∞–µ–º —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ—Å–ª–µ —ç–º–±—ç–¥–¥–∏–Ω–≥–∞
2. –î–µ–ª–∞–µ–º Positional encoding, —Å–∫–ª–∞–¥—ã–≤–µ–º –≤–µ–∫—Ç–æ—Ä—ã.
3. –°—á–∏—Ç–∞–µ–º self-attention, –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä—ã.
4. Add & Normalize –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä—ã, –∏ —Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç –∏—Ö —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –¥–æ self-attention (–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ). –î–µ–ª–∞–µ—Ç—Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LayerNorm - —Ä–∞–∑–Ω–æ–≤–∏–¥–Ω–æ—Å—Ç—å [[Batch-normalization]])
## –ó–∞–≥–ª—è–Ω–µ–º –≤–Ω—É—Ç—Ä—å –¥—ç–∫–æ–¥–µ—Ä–∞
![g](imgs/38.png)
–õ—É—á—à–µ –±—ã –Ω–µ –∑–∞–≥–ª—è–¥—ã–≤–∞–ª.